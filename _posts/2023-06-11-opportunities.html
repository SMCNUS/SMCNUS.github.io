---
permalink: /opportunities_AI_supported_language_Learning
---

<!DOCTYPE html>
<html lang="en">
    <head>
    <title>SMC Lab | Job Openings</title>
    </head>
  {% include head.html %}

  <body>
  {% include nav_sub.html %}
  <section class="bg-faded" id="Postdoc_Job_description_2022">
    <div class="container">
        <div class="row"><p class="h1"></p></div>
        
        <div class="col-lg-12 text-center">
            <h2 class="section-heading">Job Description for AI Supported Language Learning</h2>
            <hr class="primary">
        </div>

        <p class='text-justify'>The National University of Singapore (NUS) Sound and Music Computing Lab is pursuing research in Sound and Music Computing for Human Health and Potential (SMC4HHP) and has multiple research positions (postdoctoral research fellow, research assistant, as well as part-time student researcher) available. One of our key research themes is <b>AI supported language learning</b> which includes:
          
        <ul>
          <li><p> A1) <em>HCI/intelligent user interface</em> [1][2][12].</p></li>

          <li><p> A2) <em>Speech and singing voice analysis/synthesis</em> [3][4][5][6][7][8].</p></li>

          <li><p> A3) <em>Generative AI/LLM based chatbot</em> [9][10][11][12][13][14].</p></li>
        </ul>
          
        <p class='text-justify'>The paid positions can also be combined with thesis work at NUS (e.g., PhD, MComp, FYP).</p>
            

        <p class='text-justify'>Our projects generally require background in digital signal processing, AI/machine learning, natural language processing, human computer interaction.</p>

        <p class='text-justify'>If the above positions are exciting to you, please email your detailed CV and a cover letter to A/Prof. Ye WANG at <a href="mailto:wangye@comp.nus.edu.sg" target="_blank">wangye@comp.nus.edu.sg</a> with the subject title of “<b>Job in AI supported language learning</b>.” The cover letter should articulate your motivation to work with us, and which areas (A1, A2 and A3) you are particularly interested in.</p>

        <p class='text-justify'><b>References</b></p>
        <ul>
          <li><p> [1] D. Murad, R. Wang, D. Turnbull, and Y. Wang, “<a href="archive/pdf/2017-2018/2018_ACMMM18_PaperID1029.pdf" target="_blank">SLIONS: A Karaoke Application to Enhance Foreign Language Learning</a>,” in <i>Proceedings of the 26th ACM International Conference on Multimedia (<b>MM 2018</b>).</i> ACM, 2018, pp. 1679-1687.</p></li> 

            <li><p> [2] Y. Zhou, G. Percival, X. Wang, Y. Wang, and S. Zhao, “<a href="archive/pdf/2009-2011/2011_MOGCLASS-Evaluation_of_a_Collaborative_System_of_Mobile_Devices_for_Classroom_Music_Education_of_Young_Children.pdf" target="_blank">MOGCLASS: Evaluation of a Collaborative System of Mobile Devices for Classroom Music Education of Young Children</a>,” in <i>Proceedings of the International Conference on Human Factors in Computing Systems (<b>CHI 2011</b>).</i> ACM, 2011, pp. 523-532.</p></li>

            <li><p> [3] X. Wang, M. Shi, and Y. Wang, “<a href="archive/pdf/2024/2024_Interspeech_2024__Mandarin_Chinese_MDD.pdf" target="_blank">Pitch-Aware RNN-T for Mandarin Chinese Mispronunciation Detection and Diagnosis</a>,” in <i>Proceedings of the 25th Annual Conference of the International Speech Communication Association (<b>Interspeech 2024</b>).</i> ISCA, 2024, pp. 292-296.</p></li>

            <li><p> [4] H. Liu, M. Shi, and Y. Wang, “<a href="archive/pdf/2023/2023_Interspeech_zero_shot_speech_assessment.pdf" target="_blank">Zero-Shot Automatic Pronunciation Assessment</a>,” in <i>Proceedings of the 24th Annual Conference of the International Speech Communication Association (<b>Interspeech 2023</b>).</i> ISCA, 2023, pp. 1009-1013.</p></li>

            <li><p> [5] W. Wei, H. Huang, X. Gu, H. Wang, and Y. Wang, “<a href="archive/pdf/2022/2022_TMLR_camera_ready.pdf" target="_blank">Unsupervised Mismatch Localization in Cross-Modal Sequential Data with Application to Mispronunciations Localization</a>,” <i>Trans. Mach. Learn. Res. (<b>TMLR</b>),</i> vol. 2022, 2022. </p></li>

            <li><p> [6] X. Gu, L. Ou, D. Ong, and Y. Wang, “<a href="archive/pdf/2022/2022_ACM_MM_MM-ALT.pdf" target="_blank">MM-ALT: A Multimodal Automatic Lyric Transcription System</a>,” in <i>Proceedings of the 30th ACM International Conference on Multimedia (<b>MM 2022</b>).</i> ACM, 2022, pp. 3328-3337.</p></li>

            <li><p> [7] B. Sharma and Y. Wang, “<a href="archive/pdf/2019-2021/2019_SingingIntelligibility.pdf" target="_blank">Automatic Evaluation of Song Intelligibility using Singing Adapted STOI and Vocal-specific Features</a>,” <i>IEEE ACM Trans. Audio Speech Lang. Process.  (<b>TASLP</b>),</i> vol. 28, pp. 319-331, 2020.</p></li>

            <li><p> [8] Y. Wang, W. Wei, X. Gu, X. Guan, and Y. Wang, “<a href="archive/pdf/2023/2023_TASLP_PMD_CameraReady.pdf" target="_blank">Disentangled Adversarial Domain Adaptation for Phonation Mode Detection in Singing and Speech</a>,” <i>IEEE ACM Trans. Audio Speech Lang. Process.  (<b>TASLP</b>),</i> vol. 31, pp. 3746-3759, 2023. </p></li>

            <li><p> [9] J. Zhao, L. Q. Chetwin, and Y. Wang, “<a href="archive/pdf/2024/2024_SinTechSVS_IEEETrans.pdf" target="_blank">SinTechSVS: A Singing Technique Controllable Singing Voice Synthesis System</a>,” <i>IEEE ACM Trans. Audio Speech Lang. Process.  (<b>TASLP</b>),</i> vol. 32, pp. 2641–2653, 2024. </p></li>

            <li><p>[10] Q. Liang and Y. Wang, “<a href="archive/pdf/2024/2024_Drawlody__IEEE_TMM_Finalised.pdf" target="_blank">Drawlody: Sketch-Based Melody Creation with Enhanced Usability and Interpretability</a>,” <i>IEEE Trans. Multim. (<b>TMM</b>),</i> vol. 26, pp. 7074-7088, 2024.</p></li>

            <li><p>[11] X. Ma, Y. Wang, and Y. Wang, “<a href="archive/pdf/2024/2024_symbolic_music_TMM.pdf" target="_blank">Symbolic Music Generation from Graph-Learning-based Preference Modeling and Textual Queries</a>,” <i>IEEE Trans. Multim. (<b>TMM</b>),</i> vol. 26, pp. 1-14, 2024.</p></li>

            <li><p>[12] Q. Liang, X. Ma, F. Doshi-Velez, B. Lim, and Y. Wang, “<a href="archive/pdf/2024/2024_XAI_Lyricist_Human_Centred_AI_.pdf" target="_blank">XAI-Lyricist: Improving the Singability of AI-Generated Lyrics with Prosody Explanations</a>,” in <i>Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (<b>IJCAI 2024</b>).</i> ijcai.org, 2024.</p></li>

            <li><p>[13] X. Ma, Y. Wang, M. Kan, and W. S. Lee, “<a href="archive/pdf/2019-2021/2021_AI-lyricist.pdf" target="_blank">AI-Lyricist: Generating Music and Vocabulary Constrained Lyrics</a>,” in <i>Proceedings of the 29th ACM International Conference on Multimedia (<b>MM 2021</b>).</i> ACM, 2021, pp. 1002-1011.</p></li>

            <li><p>[14] H. Huang, S. Wang, H. Liu, H. Wang, and Y. Wang, “<a href="archive/pdf/2024/2024_benchmarking_large_language_mo.pdf" target="_blank">Benchmarking Large Language Models on Communicative Medical Coaching: A Dataset and a Novel System</a>,” in <i>Findings of the Association for Computational Linguistics: ACL 2024 (<b>Findings of ACL 2024</b>).</i> Association for Computational Linguistics, 2024.</p></li>



        </ul>


        </div>


        
    

</section>
  <footer class="bg-light text-center text-lg-start">
    {% include scripts.html %}
    {% include copyright.html %}
    <style>
      ul {
  list-style-type: none;
}
    </style>
  </footer>
</html>


























