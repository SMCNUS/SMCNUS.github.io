---
permalink: /iclr_speech_music_ai_workshop_2025
---

<!DOCTYPE html>
<html lang="en">
  <head>
  <title>SMC Lab | Speech and Music AI: Current Research Frontier Workshop 2025</title>
  </head>
  {% include head.html %}

  <body>
  {% include nav_sub.html %}
  <section class="bg-faded" id="Video_Page">
    <div class="container no-gutter">
      <div class="col-lg-12 text-center">
        <h2 class="section-heading">Speech and Music AI: Current Research Frontier Workshop at NUS</h2>
        <h1>&nbsp</h1>
        <hr class="primary">
      </div>

      <div class="row small-padding" id="">
        <p> </p>

        <p class='text-justify' style="text-indent: 40px">
          The Speech and Music AI: Current Research Frontier workshop was held on 28 April 2025 at the NUS School of Computing, as a side event welcoming distinguished researchers attending ICLR 2025 in Singapore. We were honoured to host professors, scientists, and students from institutions including CUHK-Shenzhen, QMUL, Belmont University, UPF, UCSD, MBZUAI, NYU, and Meta, who shared their cutting-edge research on speech and music AI technologies. The seminar was chaired by Prof. Ye Wang and was open to all faculty and students of the School of Computing.
        </p>
        <hr class="primary">

    </div>


        <div class="row no-padding" id="00_Speech_Generation_DeepFake_Detection">
          <p></p>
        </div>
        <p class="h3" style="color:#ed6502;"><b style="color:#031d83;">[01] </b>Recent Advances of Speech Generation and DeepFake Detection</p>
        <p class='text-justify' style="text-indent: 40px">
          <b>Abstract</b>: This talk will cover the recent research activities by the Amphion team led by Prof. Zhizheng Wu. The talk will focus on spoken language understanding, speech generation, unified and foundation models for speech synthesis, voice conversion, and speech enhancement. The talk will be accompanied by three highlight talks of Prof. Wu’s PhD students who presented their work at ICLR 2025 in Singapore.
        </p>
        <p class="h4" style="text-indent: 40px; color:#ed6502;"><b style="color:#031d83;">[01.1] </b> Recent Advances of Speech Generation and DeepFake Detection</p>
        <p class='text-justify' style="text-indent: 80px">
          Presented by <a href="https://drwuz.com/" target="_blank" style="color:#031d83;">Zhizheng Wu</a>, Associate Professor, Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen).
        </p>
        <p class="text-center">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/OZiyLNtd9lw?si=mAEeoJ137pM_m-VC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
        <p class="h4" style="text-indent: 40px; color:#ed6502;"><b style="color:#031d83;">[01.2] </b>Controllable and Unified Speech and Singing Voice Generation</p>
        <p class='text-justify' style="text-indent: 80px">
          Presented by <a href="https://www.zhangxueyao.com/" target="_blank" style="color:#031d83;">Xueyao Zhang</a>, PhD student, CUHK-Shenzhen.
        </p>
        <p class="text-center">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/N6vUcH20oOE?si=lUZz_8FVPfulBGss" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
        <p class="h4" style="text-indent: 40px; color:#ed6502;"><b style="color:#031d83;">[01.3] </b>MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer</p>
        <p class='text-justify' style="text-indent: 80px">
          Presented by <a href="https://hecheng0625.github.io/" target="_blank" style="color:#031d83;">Yuancheng Wang</a>, PhD student, CUHK-Shenzhen.
        </p>
        <p class="text-center">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/qHe7EQDFmKE?si=BM7J_fsIT-rQDIcF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
        <p class="h4" style="text-indent: 40px; color:#ed6502;"><b style="color:#031d83;">[01.4] </b>AnyEnhance: A Unified Generative Model with Prompt-Guidance and Self-Critic for Voice Enhancement</p>
        <p class='text-justify' style="text-indent: 80px">
          Presented by <a href="https://scholar.google.com/citations?user=r1VH6rsAAAAJ&h" target="_blank" style="color:#031d83;">Junan Zhang</a>, PhD student, CUHK-Shenzhen.
        </p>
        <p class="text-center">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/lk1Cet8B5xE?si=kl_ke7A6eO4jFLih" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
        <!--<h4 class="text-center" style="color:#85807d; font-family:Times New Roman">
          This video can be also accessed via <a href="https://www.bilibili.com/video/BV1He411A7sL" target="_blank">this link</a>, if the video above cannot be loaded. 
        </h4>-->



        <div class="row small-padding" id="01_LLMs_for_Acoustic_Music">
          <p></p>
        </div>
        <p class="h3" style="color:#ed6502;"><b style="color:#031d83;">[02]</b> Extending LLMs for Acoustic Music Understanding: Models, Benchmarks, and Multimodal Instruction Tuning</p>
        <p class='text-justify' style="text-indent: 40px">
          <b>Abstract</b>: Large Language Models (LLMs) have transformed learning and generation in text and vision, yet acoustic music—an inherently multimodal and expressive domain—remains underexplored. In this talk, I present recent progress in leveraging large-scale pre-trained models and instruction tuning for music understanding. I introduce MERT, a self-supervised acoustic music model with over 50k downloads, and MRABLE, a universal benchmark for evaluating music audio rePresenteds. I also present MusiLingo, a system that aligns pre-trained models across modalities to support music captioning and question answering. To address the gap in evaluating instruction-following capabilities, I propose CMI-Bench, the first benchmark designed to test models' ability to understand and follow complex music-related instructions across audio, text, and symbolic domains. I conclude by discussing open challenges in responsible deployment of generative music AI.
        </p>
        <p class='text-justify' style="text-indent: 40px">
          Presented by <a href="https://nicolaus625.github.io/" target="_blank" style="color:#031d83;">Yinghao Ma</a>, PhD student in AI Music, Queen Mary University (QMUL).
        </p>
        <p class="text-center">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/Rn1neXi6-8I?si=DPpkfsXteD1ltwa9" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
        <!--<h4 class="text-center" style="color:#85807d; font-family:Times New Roman">
          This video can be also accessed via <a href="https://www.bilibili.com/video/BV1LA411R7xr" target="_blank">this link</a>, if the video above cannot be loaded. 
        </h4>-->

        <div class="row small-padding" id="02_Flocoder">
          <p></p>
        </div>
        <p class="h3" style="color:#ed6502;"><b style="color:#031d83;">[03]</b> Flocoder: A Latent Flow-Matching Model for Symbolic Music Generation and Analysis</p>
        <p class='text-justify' style="text-indent: 40px">
          <b>Abstract</b>: We present work in progress on an open source framework for latent flow matching to provide generative MIDI outputs for inpainting tasks such as continuation and/or melody or accompaniment generation. This is a reframing of a prior diffusion-based system ("Pictures of MIDI", arXiv:2407.01499) for more efficient inference. A further goal is to constrain the quantized latent rePresenteds to correspond to repeated musical motifs, allowing the embeddings to be used for motif analysis. This is all presented in a new open-source framework called "flocoder" to which interested students are invited to contribute!        
        </p>
        <p class='text-justify' style="text-indent: 40px">
          Presented by <a href="https://drscotthawley.github.io/blog/about.html" target="_blank" style="color:#031d83;">Scott H. Hawley</a>, Professor, Belmont University.
        </p>
        <p class="text-center">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/hKAuBFSvVrw?si=KmVq36SawaeJYD04" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
        <!--<h4 class="text-center" style="color:#85807d; font-family:Times New Roman">
          This video can be also accessed via <a href="https://www.bilibili.com/video/BV1q8411p7oc" target="_blank">this link</a>, if the video above cannot be loaded. 
        </h4>-->

        <div class="row small-padding" id="03_AI_Enhanced_Music_Learning">
          <p></p>
        </div>
        <p class="h3" style="color:#ed6502;"><b style="color:#031d83;">[04]</b> AI-Enhanced Music Learning</p>
        <p class='text-justify' style="text-indent: 40px">
          <b>Abstract</b>: Learning to play a musical instrument is a difficult task that requires sophisticated skills. This talk will describe approaches to designing and implementing new interaction paradigms for music learning and training based on state-of-the-art AI techniques applied to multimodal (audio, video, and motion) data.
        </p>
        <p class='text-justify' style="text-indent: 40px">
          Presented by <a href="https://rafaelram.weebly.com/" target="_blank" style="color:#031d83;">Rafael Ramirez</a>, Associate Professor, Universitat Pompeu Fabra (UPF).
        </p>
        <p class="text-center">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/mtPHA-PAVdM?si=_qEUVtFRV9Ah-a_I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>        
        </p>
        <!--<h4 class="text-center" style="color:#85807d; font-family:Times New Roman">
          This video can be also accessed via <a href="https://www.bilibili.com/video/BV1q8411p7oc" target="_blank">this link</a>, if the video above cannot be loaded. 
        </h4>-->
        
        <div class="row small-padding" id="04_Presto">
          <p></p>
        </div>
        <p class="h3" style="color:#ed6502;"><b style="color:#031d83;">[05]</b> Presto! Distilling Steps and Layers for Accelerating Music Generation</p>
        <p class='text-justify' style="text-indent: 40px">
          <b>Abstract</b>: Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via preserving hidden state variance. Finally, we combine our improved step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Furthermore, we find our combined distillation method can generate high-quality outputs with improved diversity accelerating our base model by 10-18x (32 second output in 230ms, 15x faster than the comparable SOTA model) -- the fastest high-quality TTM model to our knowledge.
        </p>
        <p class='text-justify' style="text-indent: 40px">
          Presented by <a href="https://zacharynovack.github.io/" target="_blank" style="color:#031d83;">Zachary Novack</a>, PhD student, University of California San Diego (UCSD).
        </p>
        <p class="text-center">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/av07792ZmRU?si=czHhlC7n5TvAzYgs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
        <!--<h4 class="text-center" style="color:#85807d; font-family:Times New Roman">
          This video can be also accessed via <a href="https://www.bilibili.com/video/BV1q8411p7oc" target="_blank">this link</a>, if the video above cannot be loaded. 
        </h4>-->

        <div class="row no-padding" id="05_Music_X_Lab">
          <p></p>
        </div>
        <p class="h3" style="color:#ed6502;"><b style="color:#031d83;">[06] </b> Current Research at the Music X Lab</p>
        <p class='text-justify' style="text-indent: 40px">
          <b>Abstract</b>: In this talk, Gus will provide a brief overview of research at the Music X Lab, highlighting key projects at the intersection of music and artificial intelligence. He will also share reflections on the evolving landscape of Music AI in the age of large language models (LLMs), with a focus on the unique role and value of academic research in shaping its future. The talk will be accompanied by three highlight talks of Prof. Xia’s students who presented their work at ICLR 2025 in Singapore.
        </p>
        <p class="h4" style="text-indent: 40px; color:#ed6502;"><b style="color:#031d83;">[06.1] </b> Towards Meaningful (Music) AI Research</p>
        <p class='text-justify' style="text-indent: 80px">
          Presented by <a href="https://www.musicxlab.com/members/gus/" target="_blank" style="color:#031d83;">Gus Xia</a>, Associate Professor, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI).
        </p>
        <p class="text-center">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/tEoXDqorPTc?si=2d76RFFdjQuJxQZx" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
        <p class="h4" style="text-indent: 40px; color:#ed6502;"><b style="color:#031d83;">[06.2] </b> EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation</p>
        <p class='text-justify' style="text-indent: 80px">
          Presented by <a href="https://www.linkedin.com/in/xinyuelli/" target="_blank" style="color:#031d83;">Xinyue Li</a>, PhD student, MBZUAI.
        </p>
        <p class="text-center">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/sceqKfgKSR0?si=LX7jmeR4ztcndJfk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
        <p class="h4" style="text-indent: 40px; color:#ed6502;"><b style="color:#031d83;">[06.3] </b> Emergent Content-Style Disentanglement via Variance-Invariance Constraints</p>
        <p class='text-justify' style="text-indent: 80px">
          Presented by <a href="https://irislucent.github.io/" target="_blank" style="color:#031d83;">Yuxuan Wu</a>, PhD student, MBZUAI.
        </p>
        <p class="text-center">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/n7L7vwQupTI?si=DsM1DGc2RgrSS6rF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
        <p class="h4" style="text-indent: 40px; color:#ed6502;"><b style="color:#031d83;">[06.4] </b> Versatile Symbolic Music-for-Music Modeling via Function Alignment</p>
        <p class='text-justify' style="text-indent: 80px">
          Presented by <a href="https://cs.shanghai.nyu.edu/phd-students/junyan-jiang-jiangjunyan" target="_blank" style="color:#031d83;">Junyan Jiang</a>, PhD student, New York University (NYU).
        </p>
        <p class="text-center">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/WaDSzjBoKjg?si=o1n1izB9nblleEgY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
        <!--<h4 class="text-center" style="color:#85807d; font-family:Times New Roman">
          This video can be also accessed via <a href="https://www.bilibili.com/video/BV1He411A7sL" target="_blank">this link</a>, if the video above cannot be loaded. 
        </h4>-->

        <div class="row small-padding" id="06_Meta">
          <p></p>
        </div>
        <p class="h3" style="color:#ed6502;"><b style="color:#031d83;">[07]</b> Natural Language-Driven Audio Intelligence for Content Creation</p>
        <p class='text-justify' style="text-indent: 40px">
          <b>Abstract</b>: In the digital age, audio content creation has transcended traditional boundaries, becoming a dynamitic field that fuses technology, creativity, and user experience. This talk will discuss the recent advances in natural language-driven audio AI technologies that reshapes human interaction with audio content creation. In this talk, we first introduce the work on language-queried audio source separation (LASS), which aims to extract desired sounds from audio recordings using natural language queries. We’ll present AudioSep - a foundation model we proposed for separating speech, music and sound events using natural language queries. We will then discuss our first work on language-modelling & latent diffusion-based (AudioLDM) approaches for audio generation. Finally, we will introduce WavJourney, a Large Language Model (LLM) based AI agent for compositional audio creation. WavJourney is capable of crafting vivid audio storytelling with personalized voices, music, and sound effects simply from text instructions. We will further discuss the potential of our proposed compositional approach for audio generation, showing our experimental
          findings and state-of-the art results we’ve achieved on text-to-audio generation tasks.
        </p>
        <p class='text-justify' style="text-indent: 40px">
          Presented by <a href="https://liuxubo717.github.io/" target="_blank" style="color:#031d83;">Xubo Liu</a>, Research Scientist, Meta.
        </p>
        <p class="text-center">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/I62m_fKxgu4?si=JTZz414eTgdNzF23" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
        <!--<h4 class="text-center" style="color:#85807d; font-family:Times New Roman">
          This video can be also accessed via <a href="https://www.bilibili.com/video/BV1q8411p7oc" target="_blank">this link</a>, if the video above cannot be loaded. 
        </h4>-->


    </div>
    
</section>
  <footer class="bg-light text-center text-lg-start">
    {% include scripts.html %}
    {% include copyright.html %}
  </footer>
</html>
