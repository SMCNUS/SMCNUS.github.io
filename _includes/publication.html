<section class="bg-wight small-padding" id="Publication">
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">Selected Publication</h2>
                <hr class="primary">
            </div>
        </div>

    <div class="row">
        <div class="text-lg-start">
        <p class="text-center">[<a href="full_publication_list" style="color:#031d83;"  target="_blank"><em>see FULL publication list</em></a>]</p></div>
    </div>

    <div class="row">
        <div class="text-lg-start">
        <h2 class="section-heading text-uppercase">2025</h2></div> 
    </div>
    <div class="row">
        <ul>

            <li><p><p class="text-align">Q. Liang, X. Ma, T. Hopkins, and Y. Wang, “<a href="archive/pdf/2025/2025_LivePoem (IJCAI25 HAI).pdf" target="_blank">LivePoem: Improving the Learning Experience of Classical Chinese Poetry with AI-Generated Musical Storyboards</a>,” in <i>Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence (<b>IJCAI 2025</b>).</i> ijcai.org, 2025.
            [<a href="https://www.youtube.com/shorts/wXI0ITGkm04" target="_blank" style="color:#031d83;">Demo 1 (reciting)</a>]
            [<a href="https://www.youtube.com/shorts/COKSq3fguTA" target="_blank" style="color:#031d83;">Demo 1 (singing)</a>]
            [<a href="https://www.youtube.com/shorts/5aG1sn9X01s" target="_blank" style="color:#031d83;">Demo 2 (reciting)</a>]
            [<a href="https://www.youtube.com/shorts/ZaGJ4Ymh8ss" target="_blank" style="color:#031d83;">Demo 2 (singing)</a>]
            </p></li>

            <li><p class="text-align">J. Zhao, X. Wang, and Y. Wang, “<a href="archive/pdf/2025/2025_Interspeech_464.pdf" target="_blank">Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via In-Context Learning</a>,” in <i>Proceedings of the 26th Annual Conference of the International Speech Communication Association (<b>Interspeech 2025</b>).</i> ISCA, 2025s.</p></li>


            <li><p class="text-align">H. Liu, H. Huang, H. Wang, X. Gu, and Y. Wang, “<a href="archive/pdf/2025/2025_on_calibration.pdf" target="_blank"> On Calibration of LLM-based Guard Models for Reliable Content Moderation</a>,” in <em>Proceedings of the 13th International Conference on Learning Representations (<b>ICLR 2025</b>)</em>. OpenReview.net, 2025.</p></li>

            <li><p class="text-align">X. Gu, T. Pang, C. Du, Q. Liu, F. Zhang, C. Du, Y. Wang and M. Lin, “<a href="archive/pdf/2025/2025_when_attention.pdf" target="_blank">When Attention Sink Emerges in Language Models: An Empirical View</a>,” in <em>Proceedings of the 13th International Conference on Learning Representations (<b>ICLR 2025</b>)</em>. OpenReview.net, 2025.</p></li>
            
            <li><p class="text-align">X. Gu, C. Du, T. Pang, C. Li, M. Lin, and Y. Wang, “<a href="archive/pdf/2025/2025_on_memorization.pdf" target="_blank"> On Memorization in Diffusion Models</a>,” <em> Trans. Mach. Learn. Res. (<b>TMLR</b>) </em>, vol. 2025.
        
            <li><p class="text-align">L. Ou, Y. Takahashi, and Y. Wang, “<a href="archive/pdf/2025/2025_Lead_Instrument_Detection_from_Multitrack_Music_final.pdf" target="_blank"> Lead Instrument Detection from Multitrack Music </a>,” in <i>Proceedings of the 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP 2025</b>).</i> IEEE, 2025.</p></li>
        
            <li><p class="text-align">J. Zhao, C. Low, and Y. Wang, “<a href="archive/pdf/2025/2025_SPSinger.pdf" target="_blank"> SPSinger: Multi-Singer Singing Voice Synthesis with Short Reference Prompt</a>,” in <i>Proceedings of the 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP 2025</b>).</i> IEEE, 2025.</p></li>
        
        </ul>
    </div>

    <div class="row">
        <div class="text-lg-start">
        <h2 class="section-heading text-uppercase">2024</h2></div> 
    </div>
    <div class="row">
        <ul>
            <li><p class="text-align">J. Zhao, G. Xia, Z. Wang, and Y. Wang, “<a href="archive/pdf/2024/2024_structured_arrangement.pdf" target="_blank">Structured Multi-Track Accompaniment Arrangement via Style Prior Modelling</a>,” in <em>Proceedings of the 38th Annual Conference on Neural Information Processing Systems (<b>NeurIPS 2024</b>)</em>.</i> 2024.
                [<a href="https://zhaojw1998.github.io/structured-arrangement/" target="_blank" style="color:#031d83;">demo</a>] 
                [<a href="https://github.com/zhaojw1998/Structured-Arrangement-Code" target="_blank" style="color:#031d83;">code</a>] 
                </p></li> 

            <li><p class="text-align">H. Liu, H. Huang, and Y. Wang, “<a href="archive/pdf/2024/2024_SpeechTTA.pdf" target="_blank">Advancing Test-Time Adaptation in Wild Acoustic Test Settings</a>,” in <em>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (<b>EMNLP 2024</b>)</em>. Association for Computational Linguistics, 2024, pp. 7138-7155.</p></li>

            <li><p class="text-align">H. Liu*, Y. Xie*, Y. Wang, and Michael Shieh, “<a href="archive/pdf/2024/2024_DeGCG-Transfer_Attack.pdf" target="_blank">Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models</a>,” in <em>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (<b>EMNLP 2024</b>)</em>. Association for Computational Linguistics, 2024, pp. 7213-7224.</p></li>

            <li><p class="text-align">X. Ma, V. Sharma, M. Y. Kan, W. S. Lee, and Y. Wang, “<a href="archive/pdf/2024/2024_KeYric.pdf" target="_blank">KeYric: Unsupervised Keywords Extraction and Expansion from Music for Coherent Lyric Generation</a>,” <i>ACM Trans. Multim. Comput. Commun. Appl. (<b>TOMM</b>),</i>, vol. 21, No. 1, pp. 1-28, 2024. </p></li>

            <li><p class="text-align">X. Wang, M. Shi, and Y. Wang, “<a href="archive/pdf/2024/2024_Interspeech_2024__Mandarin_Chinese_MDD.pdf" target="_blank">Pitch-Aware RNN-T for Mandarin Chinese Mispronunciation Detection and Diagnosis</a>,” in <i>Proceedings of the 25th Annual Conference of the International Speech Communication Association (<b>Interspeech 2024</b>).</i> ISCA, 2024, pp. 292-296.</p></li>

            <li><p class="text-align">J. Zhao, L. Q. Chetwin, and Y. Wang, “<a href="archive/pdf/2024/2024_SinTechSVS_IEEETrans.pdf" target="_blank">SinTechSVS: A Singing Technique Controllable Singing Voice Synthesis System</a>,” <i>IEEE ACM Trans. Audio Speech Lang. Process. (<b>TASLP</b>),</i> vol. 32, pp. 2641–2653, 2024. </p></li>

            <li><p><p class="text-align">H. Huang, S. Wang, H. Liu, H. Wang, and Y. Wang, “<a href="archive/pdf/2024/2024_benchmarking_large_language_mo.pdf" target="_blank">Benchmarking Large Language Models on Communicative Medical Coaching: A Dataset and a Novel System</a>,” in <i>Findings of the Association for Computational Linguistics: ACL 2024 (<b>Findings of ACL 2024</b>).</i> Association for Computational Linguistics, 2024, pp. 1624-1637.</p></li>
                
            <li><p><p class="text-align">Q. Liang, X. Ma, F. Doshi-Velez, B. Lim, and Y. Wang, “<a href="archive/pdf/2024/2024_XAI_Lyricist_Human_Centred_AI_.pdf" target="_blank">XAI-Lyricist: Improving the Singability of AI-Generated Lyrics with Prosody Explanations</a>,” in <i>Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (<b>IJCAI 2024</b>).</i> ijcai.org, 2024, pp.7877-7885.</p></li>

            <li><p><p class="text-align">W. Zeng, X. He, and Y. Wang, “<a href="archive/pdf/2024/2024_Piano_A2S_camera_ready.pdf" target="_blank">End-to-End Real-World Polyphonic Piano Audio-to-Score Transcription with Hierarchical Decoding</a>,” in <i>Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (<b>IJCAI 2024</b>).</i> ijcai.org, 2024, pp. 7788-7795.</p></li>

            <li><p class="text-align">X. Ma, Y. Wang, and Y. Wang, “<a href="archive/pdf/2024/2024_symbolic_music_TMM.pdf" target="_blank">Symbolic Music Generation from Graph-Learning-based Preference Modeling and Textual Queries</a>,” <i>IEEE Trans. Multim. (<b>TMM</b>),</i> vol. 26, pp. 10545-10558, 2024.</p></li>

            <li><p class="text-align">X. Gu, X. Zheng, T. Pang, C. Du, Q. Liu, Y. Wang, J. Jiang, and M. Lin, “<a href="archive/pdf/2024/2024_agent_smith.pdf" target="_blank">Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast</a>,” <i>Proceedings of the 41st International Conference on Machine Learning (<b>ICML 2024</b>).</i> PMLR, 2024.</p></li>

            <li><p class="text-align">X. Gu, L. Ou, W. Zeng, J. Zhang, N. Wong, and Y. Wang, “<a href="archive/pdf/2024/2024_MMSinging_revision_final.pdf" target="_blank">Automatic Lyric Transcription and Automatic Music Transcription from Multimodal Singing</a>,” <i>ACM Trans. Multim. Comput. Commun. Appl. (<b>TOMM</b>),</i> vol. 20, No. 7, pp. 1551-6857, 2024. </p></li>

            <li><p class="text-align">Q. Liang and Y. Wang, “<a href="archive/pdf/2024/2024_Drawlody__IEEE_TMM_Finalised.pdf" target="_blank">Drawlody: Sketch-Based Melody Creation with Enhanced Usability and Interpretability</a>,” <i>IEEE Trans. Multim. (<b>TMM</b>),</i> vol. 26, pp. 7074-7088, 2024.</p></li>
        </ul>
    </div>
    

    <div class="row">
        <div class="text-lg-start">
        <h2 class="section-heading text-uppercase">2023</h2></div>
    </div>
    <div class="row">
        <ul>
            <li><p class="text-align">H. Liu and Y. Wang, “<a href="archive/pdf/2023/2023_AICL_camera_ready.pdf" target="_blank">Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning</a>,” in <em>Findings of the Association for Computational Linguistics: EMNLP 2023 (<b>Findings of EMNLP 2023</b>)</em>. Association for Computational Linguistics, 2023, pp. 15825-15838.</p></li>

            <li><p class="text-align">Y. Wang, W. Wei, X. Gu, X. Guan, and Y. Wang, “<a href="archive/pdf/2023/2023_TASLP_PMD_CameraReady.pdf" target="_blank">Disentangled Adversarial Domain Adaptation for Phonation Mode Detection in Singing and Speech</a>,” <i>IEEE ACM Trans. Audio Speech Lang. Process.  (<b>TASLP</b>),</i> vol. 31, pp. 3746–3759, 2023. </p></li>

            <li><p class="text-align">X. Gu, W. Zeng, and Y. Wang, “<a href="archive/pdf/2023/2023_ACM_MM2023_Fairness_Singing_camera_ready.pdf" target="_blank">Elucidate Gender Fairness in Singing Voice Transcription</a>,” in <i>Proceedings of the 31st ACM International Conference on Multimedia (<b>MM 2023</b>).</i> ACM, 2023, pp. 8760–8769.</p></li>

            <li><p><p class="text-align">H. Liu, M. Shi, and Y. Wang, “<a href="archive/pdf/2023/2023_Interspeech_zero_shot_speech_assessment.pdf" target="_blank">Zero-Shot Automatic Pronunciation Assessment</a>,” in <i>Proceedings of the 24th Annual Conference of the International Speech Communication Association (<b>Interspeech 2023</b>).</i> ISCA, 2023, pp. 1009–1013.</p></li>

            <li><p><p class="text-align">J. Zhao, G. Xia, and Y. Wang, “<a href="archive/pdf/2023/2023_IJCAI_music_rearrangement.pdf" target="_blank">Q&A: Query-Based Representation Learning for Multi-Track Symbolic Music re-Arrangement</a>,” in <i>Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (<b>IJCAI 2023</b>).</i> ijcai.org, 2023, pp. 5878–5886. 
            [<a href="https://github.com/zhaojw1998/Query-and-reArrange" target="_blank" style="color:#031d83;">code</a>] 
            [<a href="https://zhaojw1998.github.io/Query_and_reArrange" target="_blank" style="color:#031d83;">demo</a>] 
            [<a href="https://colab.research.google.com/drive/1N3XeEfTCWNLTuBp9NWPwzW-hq7Ho7nQA?usp=sharing" target="_blank" style="color:#031d83;">tutorial</a>]</p></li>

            <li><p><p class="text-align">L. Ou, X. Ma, M. Kan, and Y. Wang, “<a href="archive/pdf/2023/2023_ACL_Lyric_Translation.pdf" target="_blank">Songs Across Borders: Singable and Controllable Neural Lyric Translation</a>,” in <i>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (<b>ACL 2023</b>).</i> Association for Computational Linguistics, 2023, pp. 447–467. 
            [<a href="https://github.com/Sonata165/ControllableLyricTranslation" target="_blank" style="color:#031d83;">code</a>] 
            [<a href="https://www.oulongshen.xyz/lyric_translation" target="_blank" style="color:#031d83;">demo</a>]</p></li>

            <li><p><p class="text-align">Y. Wang, W. Wei, and Y. Wang, “<a href="archive/pdf/2023/2023_ICASSP_phonation_mode.pdf" target="_blank">Phonation Mode Detection in Singing: A Singer Adapted Model</a>,” in <i>Proceedings of the 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP 2023</b>).</i> IEEE, 2023, pp. 1–5.</p></li>
        </ul>
    </div>
    

    <div class="row">
        <div class="text-lg-start">
        <p class="text-center">[<a href="full_publication_list" style="color:#031d83;"  target="_blank"><em>see FULL publication list</em></a>]</p></div>
    </div>

    </div>
















    </div>
</section>
