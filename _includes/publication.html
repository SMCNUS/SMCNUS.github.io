<section class="bg-wight small-padding" id="Publication">
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">Selected Publication</h2>
                <hr class="primary">
            </div>
        </div>

    <div class="row">
        <div class="text-lg-start">
        <p class="text-center">[<a href="full_publication_list" style="color:#031d83;"  target="_blank"><em>see FULL publication list</em></a>]</p></div>
    </div>


    <div class="row">
        <div class="text-lg-start">
        <h2 class="section-heading text-uppercase">2024</h2></div>
    </div>
    <div class="row">
        <ul>
            <li><p class="text-align">X. Gu, L. Ou, W. Zeng, J. Zhang, N. Wong, and Y. Wang, “<a href="archive/pdf/2024/2024_MMSinging_revision_final.pdf" target="_blank">Automatic Lyric Transcription and Automatic Music Transcription from Multimodal Singing</a>,” <i>ACM Trans. Multim. Comput. Commun. Appl. (<b>TOMM</b>),</i> vol. 20, pp. 1-28, 2024. </p></li>

            <li><p class="text-align">Q. Liang and Y. Wang, “<a href="archive/pdf/2024/2024_Drawlody__IEEE_TMM_Finalised.pdf" target="_blank">Drawlody: Sketch-Based Melody Creation with Enhanced Usability and Interpretability</a>,” <i>IEEE Trans. Multim. (<b>TMM</b>),</i> vol. 26, pp. 1-14, 2024.</p></li>
        </ul>
    </div>
    

    <div class="row">
        <div class="text-lg-start">
        <h2 class="section-heading text-uppercase">2023</h2></div>
    </div>
    <div class="row">
        <ul>

            <li><p class="text-align">H. Liu and Y. Wang, “<a href="archive/pdf/2023/2023_AICL_camera_ready.pdf" target="_blank">Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning</a>,” in <em>Findings of the Association for Computational Linguistics: EMNLP 2023 (<b>Findings of EMNLP 2023</b>)</em>. Association for Computational Linguistics, 2023, pp. 15825-15838.</p></li>

            <li><p class="text-align">Y. Wang, W. Wei, X. Gu, X. Guan, and Y. Wang, “<a href="archive/pdf/2023/2023_TASLP_PMD_CameraReady.pdf" target="_blank">Disentangled Adversarial Domain Adaptation for Phonation Mode Detection in Singing and Speech</a>,” <i>IEEE ACM Trans. Audio Speech Lang. Process.  (<b>TASLP</b>),</i> vol. 31, pp. 3746–3759, 2023. </p></li>

            <li><p class="text-align">X. Gu, W. Zeng, and Y. Wang, “<a href="archive/pdf/2023/2023_ACM_MM2023_Fairness_Singing_camera_ready.pdf" target="_blank">Elucidate Gender Fairness in Singing Voice Transcription</a>,” in <i>Proceedings of the 31st ACM International Conference on Multimedia (<b>MM 2023</b>).</i> ACM, 2023, pp. 8760–8769.</p></li>

            <li><p><p class="text-align">H. Liu, M. Shi, and Y. Wang, “<a href="archive/pdf/2023/2023_Interspeech_zero_shot_speech_assessment.pdf" target="_blank">Zero-Shot Automatic Pronunciation Assessment</a>,” in <i>Proceedings of the 24th Annual Conference of the International Speech Communication Association (<b>Interspeech 2023</b>).</i> ISCA, 2023, pp. 1009–1013.</p></li>

            <li><p><p class="text-align">J. Zhao, G. Xia, and Y. Wang, “<a href="archive/pdf/2023/2023_IJCAI_music_rearrangement.pdf" target="_blank">Q&A: Query-Based Representation Learning for Multi-Track Symbolic Music re-Arrangement</a>,” in <i>Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (<b>IJCAI 2023</b>).</i> ijcai.org, 2023, pp. 5878–5886. 
            [<a href="https://github.com/zhaojw1998/Query-and-reArrange" target="_blank" style="color:#031d83;">code</a>] 
            [<a href="https://zhaojw1998.github.io/Query_and_reArrange" target="_blank" style="color:#031d83;">demo</a>] 
            [<a href="https://colab.research.google.com/drive/1N3XeEfTCWNLTuBp9NWPwzW-hq7Ho7nQA?usp=sharing" target="_blank" style="color:#031d83;">tutorial</a>]</p></li>

            <li><p><p class="text-align">L. Ou, X. Ma, M. Kan, and Y. Wang, “<a href="archive/pdf/2023/2023_ACL_Lyric_Translation.pdf" target="_blank">Songs Across Borders: Singable and Controllable Neural Lyric Translation</a>,” in <i>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (<b>ACL 2023</b>).</i> Association for Computational Linguistics, 2023, pp. 447–467. 
            [<a href="https://github.com/Sonata165/ControllableLyricTranslation" target="_blank" style="color:#031d83;">code</a>] 
            [<a href="https://www.oulongshen.xyz/lyric_translation" target="_blank" style="color:#031d83;">demo</a>]</p></li>

            <li><p><p class="text-align">Y. Wang, W. Wei, and Y. Wang, “<a href="archive/pdf/2023/2023_ICASSP_phonation_mode.pdf" target="_blank">Phonation Mode Detection in Singing: A Singer Adapted Model</a>,” in <i>Proceedings of the 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP 2023</b>).</i> IEEE, 2023, pp. 1–5.</p></li>
        </ul>
    </div>


    <div class="row">
        <div class="text-lg-start">
            <h2 class="section-heading text-uppercase">2022</h2>
        </div>
        <ul>
            <li><p class="text-align">W. Wei, H. Huang, X. Gu, H. Wang, and Y. Wang, “<a href="archive/pdf/2022/2022_TMLR_camera_ready.pdf" target="_blank">Unsupervised Mismatch Localization in Cross-Modal Sequential Data with Application to Mispronunciations Localization</a>,” <i>Trans. Mach. Learn. Res. (<b>TMLR</b>),</i> vol. 2022, 2022. </p></li>

            <li><p class="text-align">L. Ou, X. Gu, and Y. Wang, “<a href="archive/pdf/2022/2022_ALT_ISMIR_2022_Camera_ready 8.23.pdf" target="_blank">Transfer Learning of Wav2Vec 2.0 for Automatic Lyric Transcription</a>,” in <i>Proceedings of the 23rd International Society for Music Information Retrieval Conference (<b>ISMIR 2022</b>).</i> 2022, pp. 891-899. </p></li>
    
            <li><p class="text-align">X. Gu, L. Ou, D. Ong, and Y. Wang, “<a href="archive/pdf/2022/2022_ACM_MM_MM-ALT.pdf" target="_blank">MM-ALT: A Multimodal Automatic Lyric Transcription System</a>,” in <i>Proceedings of the 30th ACM International Conference on Multimedia (<b>MM 2022</b>).</i> ACM, 2022, pp. 3328-3337. (<b>Top Paper Award</b>)
            [<a href="https://n20em.github.io/" target="_blank" style="color:#031d83;">demo</a>] </p></li>
    
            <li><p class="text-align">X. Ma, Y. Wang, and Y. Wang, “<a href="archive/pdf/2022/2022_ACM_MM_User_Preference_Modeling.pdf" target="_blank">Content Based User Preference Modeling in Music Generation</a>,” in <i>Proceedings of the 30th ACM International Conference on Multimedia (<b>MM 2022</b>).</i> ACM, 2022, pp. 2473-2482. 
            [<a href="archive/demo/stan/2022_MusicRx-C_Badinerie_music_generated_0.6_0.6_0.6_0.6.mp3" target="_blank" style="color:#031d83;">demo 1</a>] 
            [<a href="archive/demo/stan/2022_MusicRx-C_Imagine_music_generated_100_0.6_0.6_1.0_0.6.mp3" target="_blank" style="color:#031d83;">demo 2</a>]</p></li>
    </div>

    <div class="row">
        <div class="text-lg-start">
        <h2 class="section-heading text-uppercase">2021</h2></div>
    </div>
    <div class="row">
    <ul>
        <li><p>X. Ma, Y. Wang, M. Kan, and W. S. Lee, “<a href="archive/pdf/2019-2021/2021_AI-lyricist.pdf" target="_blank">AI-Lyricist: Generating Music and Vocabulary Constrained Lyrics</a>,” in <i>Proceedings of the 29th ACM International Conference on Multimedia (<b>MM 2021</b>).</i> ACM, 2021, pp. 1002–1011.
            [<a href="archive/demo/stan/2021_AI-Lyricist_Imagine_lyrics_generated.pdf" target="_blank" style="color:#031d83;">lyrics demo</a>] 
            [<a href="archive/demo/stan/2021_AI-Lyricist_Imagine_lyrics_generated.mp3" target="_blank" style="color:#031d83;">synthsis demo</a>]</p></li>
    </ul>
    </div>

    <div class="row">
        <div class="text-lg-start">
            <h2 class="section-heading text-uppercase">2020</h2>
        </div>
        <ul>
            <li><p>W. Wei, H. Zhu, E. Benetos, and Y. Wang, “<a href="archive/pdf/2019-2021/2020_A_CRNN_ICASSP_2020.pdf" target="_blank">A-CRNN: A Domain Adaptation Model for Sound Event Detection</a>,” in <i>Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP 2020</b>).</i> IEEE, 2020, pp. 276-280.</p></li>
        </ul>
    </div> 
    

    

    <div class="row">
        <div class="text-lg-start">
        <p class="text-center">[<a href="full_publication_list" style="color:#031d83;"  target="_blank"><em>see FULL publication list</em></a>]</p></div>
    </div>

    </div>
















    </div>
</section>
